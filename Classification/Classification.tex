\documentclass[12pt]{article}
% add some essential packages, some might not be used

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames]{color}
\usepackage{natbib}
\usepackage{authblk}
\usepackage{ragged2e}
\usepackage{amsmath}
\usepackage[a4paper,margin=1in,bottom=1.0in]{geometry}
\usepackage{url}
\usepackage{array}
\usepackage{bbding}
\usepackage{amssymb}
\usepackage{graphicx}  % mini page function
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{float}
\usepackage{appendix} % appendix package
\usepackage{hyperref}
\usepackage{url}
\usepackage[english]{babel}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{textgreek}
\usepackage{bibentry}
\nobibliography*
\usepackage{lipsum}


\usepackage{listings}
\usepackage{wasysym}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{bm}
\usepackage{booktabs}  % package for table line
% \usepackage{amsrefs?}  % ams citation style package


\usepackage{rotating} % for the horizontal page table

\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{matrix}
\usetikzlibrary{positioning}
\usepackage{color}
\usepackage{setspace}
\usepackage{xcolor}

\usepackage{tcolorbox} % package for making colorful box

 \setlength{\parskip}{0.15cm} % change the paragraph spacing
\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$} % set the bullet size as tiny

% \newcommand*\rot{\rotatebox{90}} % for rotate text

\usepackage{sectsty} %package for section size

\sectionfont{\fontsize{14}{12}\selectfont} % Change the section font size

\subsectionfont{\fontsize{13}{12}\selectfont}
\subsubsectionfont{\fontsize{12}{12}\selectfont}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % new command



\theoremstyle{definition}
\newtheorem{definition}[subsection]{Definition}
\newtheorem{axiom}[subsection]{Axiom}
\newtheorem{example}[subsubsection]{Example}
\newtheorem{theorem}[subsection]{Theorem}
\newtheorem{proposition}[subsection]{Proposition}
\newtheorem{lemma}[subsection]{Lemma}


\usepackage{courier}

% tikzsetting

\usetikzlibrary{shapes,decorations,arrows,calc,arrows.meta,fit,positioning}

\tikzset{
    -Latex,auto,node distance =1 cm and 1 cm,semithick,
    state/.style ={ellipse, draw, minimum width = 0.7 cm},
    point/.style = {circle, draw, inner sep=0.04cm,fill,node contents={}},
    bidirected/.style={Latex-Latex,dashed},
    el/.style = {inner sep=2pt, align=left, sloped}
}

\lstset{language=Python}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{RGB}{145, 153, 165}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{gray},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{RoyalBlue},       % keyword style
  language=Python,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}


% Define colors
\definecolor{cmd}{HTML}{F7F7F9}
\DeclareMathOperator{\di}{d\!}

\newcommand{\pr}{$\mathbb{P}$}
\newcommand{\pre}{\mathbb{P}}

\begin{document}

\title{Classification: Logistic Regression and Linear Discriminant Analysis}
\author{Michael}
\date{}
\maketitle


\section{Intuition Behind Logistic Regression}

In statistics 101 or econometrics 101, students are taught that they should be aware of the properties of independent variables. For instance, when we want to study the effects of different factors, such as education, gender, family background, etc., on individual's income level, we have to know that gender is dummy variable, education is categorical variables, etc. We don't care too much whether independent variables are continuous or not (e.g., dummy variable is not continuous) as long as dependent variable ($Y$) is continuous. Why? Because we need $Y$ is continuous when we want to do differentiation to find the optimal solutions.

What if dependent variable $Y$ now becomes discrete values, such as 1, 2, 3, 4, which they are not continuous? We need employ the logistic transformation to bring the model into a continuous domain. Through the logistic transformation, we \textit{link our categorical values(like class A, B, C) with certain probability values ($\pi \in [0, 1]$)}.


Now, we would like to have the probability $\pi_i$ depend on a vector of observed covariates $x_i$ (e.g., education, income). The simplest idea would be to let $\pi_i$ be a linear function of the covariates, say
\begin{equation}
	\pi_i = x_i'\beta
\end{equation}
where $\beta$ is a vector of regression coefficients. This model sometimes is also called the linear probability model. Once we get the estimated probability $\hat{\pi}$, if we can compare it with benchmark value, say if $\hat{\pi} > 0.6$ then observation with $x_i$ characters can be classified as class A.

However, one problem with this model is that probability $\pi_i$ on the left hand side has to be between 0 and 1, but the linear predictor $x_i'\beta$ on the right hand side can take any real value, so there is no guarantee that the predicted values will be in the correct range unless complex restrictions are imposed on the coefficients.

A simple solution to this problem is to transform the probability to remove the range restrictions, and model the transformation as a linear function of the covariates. We do this in two steps:
\begin{enumerate}
	\item  we move from the probability $\pi$ to the odds: $$odds_i = \frac{\pi_i}{1-\pi_i}$$
	\item we take logarithms, calculating the $logit$ or log-odds: $$\eta_i = \log(odds) = \log (\frac{\pi_i}{1-\pi_i}), where \ \eta \in (-\infty, + \infty)$$
\end{enumerate}

Now, Now, let's do some simple algebra and begin to use MLE. Solving for $\pi_i$ in above equation, it gives
\begin{equation}
	\pi_i = logit^{-1}(\eta_i) = \frac{e^{\eta_i}}{1+e^{\eta_i}}
\end{equation}

We are now in a position to define the logistic regression model, by assuming that the logit of the odds ratio, rather than the probability itself, follows a linear model:
\begin{equation}
	log(\frac{\pi_i}{1-\pi_i}) = x_i' \beta \ \ \Rightarrow \frac{\pi_i}{1-\pi_i} = e^{x_i'\beta} \ \ \Rightarrow \pi_i = F(x_i'\beta) = \frac{e^{x_i'\beta}}{1+e^{x_i'\beta}}
\end{equation}

In this case marginal effect can be obtained as
\begin{equation}
	\frac{d\pi_i}{dx_{ij}} = \Big[ \frac{e^{x_i'\beta}}{(1+e^{x_i'\beta})^2} \Big]\beta_j = [\frac{e^{x_i'\beta }}{(1+e^{x_i'\beta})} \frac{1}{(1+e^{x_i'\beta})} ] \beta_j = \beta_j \pi_i(1-\pi_i)
\end{equation}

\section{Logistic Regression}

Most of content in this section is taken from the notes by \cite{ng2014cs229}. As it has shown in (1.3), we can get a continuous function (or map) between covariates $x$ and dependent variable probability $\pi$:
\begin{align}
  \pi = h_{\theta}(x) = \frac{e^{\theta' x}}{ 1 + e^{\theta' x}} = \frac{1}{1+ e^{-\theta' x}}
\end{align}
where the general format
\begin{align*}
  g(z) = \frac{1}{1+e^{-z}}
\end{align*}
is called the \textbf{logistic function}.

So, given the logistic regression model, how do we fit $\theta$ for it? As we are using the probability now, we need employ the maximum likelihood. Let us assume that
\begin{align*}
  P(y = 1 | x; \theta) & = h_{\theta}(x) \\
  P(y = 0 |x; \theta) & = 1 - h_{\theta}(x)
\end{align*}
Note this can be written more compactly as
\begin{align}
  P(y |x; \theta) = (h_{\theta}(x))^y (1 - h_{\theta}(x))^{1-y}
\end{align}
Assuming that the m training examples were generated independently, we can then write down the likelihood of the parameters as
\begin{align*}
  L(\theta) & = \prod_{i=1}^m P(y^{i}| x^{i}; \theta) \\
  & = \prod_{i=1}^m (h_{\theta}(x))^y (1 - h_{\theta}(x))^{1-y}
\end{align*}

How do we maximize the likelihood? We can still use gradient descent method. However, as we are doing maximum of likelihood rather than the minimum of cost function, so our update rule will become (we also call descent ascent not descent)
\begin{align*}
  \theta = \theta + \alpha \Delta_{\theta} l (\theta)
\end{align*}
where $\Delta_{\theta} l (\theta)$ is the just derivative of our likelihood function. Now, let's take one training example (x, y) and take derivative:
\begin{align*}
  \frac{\partial L(\theta)}{\partial \theta} & = \bigg(y \frac{1}{g(\theta' x)} - (1 - y) \frac{1}{1 - g(\theta' x)} \bigg) \frac{\partial }{\partial \theta_j} g(\theta' x) \\
  & = (y - h_{\theta}(x)) x_j \\
  & = (y - \frac{1}{1+e^{-\theta' x}})x_j
\end{align*}
This therefore gives us the \textit{stochastic gradient ascent} rule
\begin{align}
  \theta_j = \theta_j + \alpha (y^i - h_\theta(x^i))x_j^i
\end{align}
where
\begin{align*}
   h_{\theta}(x) = \frac{e^{\theta' x}}{ 1 + e^{\theta' x}} = \frac{1}{1+ e^{-\theta' x}}
\end{align*}

If we have more than two categories to classify, we can still use logistic regression to model the classification. However, the method is not as efficient as linear discriminant analysis, which we will go through in the next section.

\section{Linear Discriminant Analysis}

To employ the logistic regression in section 2 we need know the class posteriors $P(y| x; \theta)$ for optimal classification. For instance, we are using Bernoulli distribution in equation (2.2). Now, suppose $f_k(x)$ is the class-conditional density of $X$ in class $G = k$, and let $\pi_k$ be the prior probability of class k, with $\sum_{k = 1}^K \pi_k = 1$. A simple application of Bayes theorem gives us
\begin{align}
  P(G = k | X = x) = \frac{f_k(x) \pi_k}{\sum_{l = 1}^K f_l(x) \pi_l}
\end{align}

We see that in terms of ability to classify, having the $f_k(x)$ is almost equivalent to having the quantity $P(G = k |X = x)$. Many techniques are based on models for the class densities:
\begin{itemize}
  \item linear and quadratic discriminant analysis use Gaussian densities
  \item more flexible mixtures of Gaussians allow for nonlinear decision boundaries
  \item general nonparametric density estimates for each class density allow the most flexibility.
\end{itemize}

\noindent
\textbf{Remark}: Andrew Ng wrote the brilliant notes for this section, please read it. 





\newpage
\bibliography{/Users/Michael/Documents/MachineLearning/ML.bib}
\bibliographystyle{apalike}
\end{document}
